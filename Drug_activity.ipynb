{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbSoIKJEVJvsfEJxL0+RI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishikakumari2000/Self-Project/blob/main/Drug_activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5WAn1PIPIpu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "fh1 = open('train.dat', 'r')\n",
        "fh2 = open('test.dat', 'r')\n",
        "trainData = fh1.readlines()\n",
        "testData = fh2.readlines()\n",
        "classes = [] # stores the test classes\n",
        "fin = np.zeros((9, 350)) # stores the output matrix generated\n",
        "\n",
        "# create a sparse matrix out of train data\n",
        "def get_train(data):\n",
        "    sparse_arr = np.zeros((800, 100001))\n",
        "    counter = 0\n",
        "\n",
        "    for i in data:\n",
        "        splitted = i.split(' ')\n",
        "        splitted = splitted[:-1]  # removing the newline character\n",
        "        zero_element = splitted[0].split('\\t')\n",
        "        splitted[0] = zero_element[1]\n",
        "        classes.append(int(zero_element[0]))  # storing classes\n",
        "\n",
        "        for y in splitted:\n",
        "            y = int(y)\n",
        "            sparse_arr[counter, y] = 1\n",
        "\n",
        "        counter = counter + 1\n",
        "\n",
        "    return sparse_arr\n",
        "\n",
        "# create a sparse matrix out of test data\n",
        "def get_test(data):\n",
        "    sparse_arr = np.zeros((350, 100001))\n",
        "    counter = 0\n",
        "\n",
        "    for i in data:\n",
        "        splitted = i.split(' ')\n",
        "        splitted = splitted[:-1]\n",
        "\n",
        "        for y in splitted:\n",
        "            y = int(y)\n",
        "            sparse_arr[counter, y] = 1\n",
        "\n",
        "        counter = counter + 1\n",
        "\n",
        "    return sparse_arr\n",
        "\n",
        "\n",
        "train_sparse = get_train(trainData)\n",
        "test_sparse = get_test(testData)\n",
        "\n",
        "train_pruned = np.zeros((156, 100001)) # contains the data used while classification\n",
        "train_zeros = np.zeros((722, 100001)) # contains records with 0 class\n",
        "train_unity = np.zeros((78, 100001)) # contains records with 1 class\n",
        "zero_holder = np.zeros((78, 100001)) # holds the 0 record buckets of size 78\n",
        "\n",
        "counter = 0\n",
        "class_pruned = [0]*156 # contains classes for train_pruned\n",
        "for x in range(78):\n",
        "    class_pruned[x] = 0\n",
        "    class_pruned[x + 78] = 1\n",
        "\n",
        "for x in range(800): # get the 0 class records\n",
        "    if classes[x] == 0:\n",
        "        train_zeros[counter] = train_sparse[x]\n",
        "        counter = counter + 1\n",
        "\n",
        "counter = 0\n",
        "for x in range(800): # get the 1 class records\n",
        "    if classes[x] == 1:\n",
        "        train_unity[counter] = train_sparse[x]\n",
        "        counter = counter + 1\n",
        "\n",
        "# using PCA for dimensionality reduction with size 300(350 also gave the same result)\n",
        "pca = PCA(n_components=300)\n",
        "# used SVC with linear kernel for classification\n",
        "clf = SVC(kernel='linear', random_state=25)\n",
        "one_pruned = train_unity[:78, :]\n",
        "\n",
        "for x in range(78):\n",
        "    train_pruned[x + 78] = one_pruned[x] # last 78 records are of class 1, first 78 will be of class 0\n",
        "\n",
        "inc = 78\n",
        "for x in range(9): # creating 9 buckets of class 0 data(each contains 78 records), and using each bucket one by one for classification\n",
        "    zero_holder = train_zeros[inc - 78:inc, :]\n",
        "    inc = inc + 78\n",
        "    for y in range(78):\n",
        "        train_pruned[y] = zero_holder[y] # inserting 0 records in train_pruned\n",
        "\n",
        "    pca.fit_transform(train_pruned)\n",
        "    clf.fit(train_pruned, class_pruned)\n",
        "\n",
        "    ans = clf.predict(test_sparse)\n",
        "    #     print ans\n",
        "    fin[x, :] = ans\n",
        "\n",
        "sum_ans = fin.sum(axis = 0)\n",
        "print sum_ans\n",
        "f = open('format.dat', 'w')\n",
        "for x in sum_ans: # if a record has been classified as 1 in any iteration, classify it as 1 in the final classification\n",
        "    if x > 0.0:\n",
        "        f.write(str(1))\n",
        "        f.write('\\n')\n",
        "    else:\n",
        "        f.write(str(0))\n",
        "        f.write('\\n')\n",
        "\n",
        "f.close()"
      ]
    }
  ]
}